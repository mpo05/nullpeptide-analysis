{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Imports for data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Imports for efficient string matching\n",
    "import ahocorasick\n",
    "\n",
    "# Imports for progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Imports for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "\n",
    "# Imports for data processing and statistical analysis\n",
    "from scipy import stats\n",
    "\n",
    "# Imports for network analysis and adjacency matrices analysis\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working paths.\n",
    "working_dir = Path(\"path/to/working/directory\") #Change this with you working directory\n",
    "\n",
    "DMS_files_path = working_dir/\"DMS\"\n",
    "DMS_samples_with_subs_path = DMS_files_path/\"Substitutions\"\n",
    "DMS_samples_with_indels_path = DMS_files_path/\"Indels\"\n",
    "\n",
    "# Set nullpeptides path\n",
    "nullpeptides_path = working_dir/\"nullpeptides\"\n",
    "\n",
    "# Read files for analysis as pandas dataframes.\n",
    "DMS_samples_with_subs_info_df = pd.read_csv(DMS_files_path/\"DMS_substitutions.csv\")\n",
    "DMS_samples_with_indels_info_df = pd.read_csv(DMS_files_path/\"DMS_indels.csv\")\n",
    "\n",
    "# Keep only Human experiments.\n",
    "human_only_DMS_samples_with_subs_info_df = DMS_samples_with_subs_info_df[DMS_samples_with_subs_info_df['taxon'] == 'Human']\n",
    "human_only_DMS_samples_with_indels_info_df = DMS_samples_with_indels_info_df[DMS_samples_with_indels_info_df['taxon'] == 'Human']\n",
    "\n",
    "# Save Human only analyses to csv.\n",
    "human_only_DMS_samples_with_subs_info_df.to_csv(DMS_files_path/\"human_only_DMS_substitutions.csv\")\n",
    "human_only_DMS_samples_with_indels_info_df.to_csv(DMS_files_path/\"human_only_DMS_indels.csv\")\n",
    "\n",
    "# Set paths to save human samples.\n",
    "human_DMS_samples_with_subs_path = DMS_samples_with_subs_path/\"human_samples\"\n",
    "human_DMS_samples_with_indels_path = DMS_samples_with_indels_path/\"human_samples\"\n",
    "\n",
    "# Define an output directory for each mutation type.\n",
    "modified_subs_path = human_DMS_samples_with_subs_path/\"modified_human_samples\"\n",
    "modified_indels_path = human_DMS_samples_with_indels_path/\"modified_human_samples\"\n",
    "\n",
    "# Specify empty lists to store the dataframe for each sample.\n",
    "modified_human_DMS_samples_with_subs_list = []\n",
    "modified_human_DMS_samples_with_indels_list = []\n",
    "\n",
    "# Specify the output directories for each mutation case.\n",
    "plots_directory = DMS_files_path/\"Plots\"\n",
    "networks_directory = DMS_files_path/\"Networks\"\n",
    "\n",
    "# Specify the output directories for each mutation case.\n",
    "subs_output_directory = plots_directory/\"Subs\"\n",
    "indels_output_directory = plots_directory/\"Indels\"\n",
    "\n",
    "# Set the style for all plots\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Skip SettingWithCopyWarnings\n",
    "pd.set_option('mode.chained_assignment', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(path):\n",
    "    \"\"\"\n",
    "    Creates a directory at the specified path if it does not already exist.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def copy_human_samples(source_path, destination_path):\n",
    "    \"\"\"\n",
    "    Copies files containing '_HUMAN_' in their name from the source path to the destination path.\n",
    "    \"\"\"\n",
    "    \n",
    "    for filename in os.listdir(source_path):\n",
    "        if \"_HUMAN_\" in filename:\n",
    "            source_file = os.path.join(source_path, filename)\n",
    "            destination_file = os.path.join(destination_path, filename)\n",
    "            shutil.copy(source_file, destination_file)\n",
    "\n",
    "# Create paths if they don't exist\n",
    "create_directory(human_DMS_samples_with_subs_path)\n",
    "create_directory(human_DMS_samples_with_indels_path)\n",
    "create_directory(modified_subs_path)\n",
    "create_directory(modified_indels_path)\n",
    "create_directory(networks_directory)\n",
    "create_directory(subs_output_directory)\n",
    "create_directory(indels_output_directory)\n",
    "\n",
    "# Copy human DMS samples to their respective new folders\n",
    "copy_human_samples(DMS_samples_with_subs_path, human_DMS_samples_with_subs_path)\n",
    "copy_human_samples(DMS_samples_with_indels_path, human_DMS_samples_with_indels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(DMS_info, source_path, destination_path, filename, mutation_type):\n",
    "    \"\"\"\n",
    "    For a given DMS sample file, adds a new row with the wild type sequence and two new columns (Protein and Study Type) and saves the modified DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    file_path = os.path.join(source_path, filename)\n",
    "\n",
    "    # Extract information based on the DMS filename\n",
    "    DMS_file_info = DMS_info[DMS_info['DMS_filename'] == filename]\n",
    "\n",
    "    if not DMS_file_info.empty:\n",
    "        target_seq = DMS_file_info['target_seq'].values[0]\n",
    "        DMS_binarization_cutoff = DMS_file_info['DMS_binarization_cutoff'].values[0]\n",
    "        coarse_selection_type = DMS_file_info['coarse_selection_type'].values[0]\n",
    "        protein_name = filename.split('_')[0]  # Assuming protein name is always the first part before the first underscore\n",
    "\n",
    "        DMS_df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Add new columns for protein name and study type\n",
    "        DMS_df['Protein'] = protein_name\n",
    "        DMS_df['Study_Type'] = coarse_selection_type\n",
    "        \n",
    "        # Define a new row to be added for the wild type\n",
    "        if mutation_type == 'subs':\n",
    "            new_row = {'mutant': 'Wild Type', 'mutated_sequence': target_seq, 'DMS_score': DMS_binarization_cutoff, 'DMS_score_bin': 1}\n",
    "        elif mutation_type == 'indels':\n",
    "            new_row = {'mutant': target_seq, 'DMS_score': DMS_binarization_cutoff, 'DMS_score_bin': 1}\n",
    "        \n",
    "        DMS_df = pd.concat([pd.DataFrame([new_row]), DMS_df], ignore_index=True)\n",
    "\n",
    "        # Save the updated DataFrame to a new CSV file\n",
    "        modified_filename = os.path.join(destination_path, f'modified_{filename}')\n",
    "        DMS_df.to_csv(modified_filename, index=False)\n",
    "\n",
    "\n",
    "# Process substitution samples\n",
    "for filename in os.listdir(human_DMS_samples_with_subs_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        add_features(human_only_DMS_samples_with_subs_info_df, human_DMS_samples_with_subs_path, modified_subs_path, filename, 'subs')\n",
    "\n",
    "# Process indel samples\n",
    "for filename in os.listdir(human_DMS_samples_with_indels_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        add_features(human_only_DMS_samples_with_indels_info_df, human_DMS_samples_with_indels_path, modified_indels_path, filename, 'indels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to scale all DMS protein fitness scores to specific scale.\n",
    "def scale_DMS_scores(data, score_column):\n",
    "    \"\"\"\n",
    "    Calculate a modified Z-score for each mutation, using the DMS score of the wild type sequence instead of the mean. This centers the data around the wild type score\n",
    "    and calculates the deviations from this score.\n",
    "    \"\"\"\n",
    "    \n",
    "    wild_type_score = data.loc[0, score_column]\n",
    "    \n",
    "    # Calculate deviations from the wild type score\n",
    "    deviations = data[score_column] - wild_type_score\n",
    "    \n",
    "    # Square these deviations\n",
    "    squared_deviations = deviations ** 2\n",
    "    \n",
    "    # Calculate the mean of the squared deviations\n",
    "    mean_squared_deviation = squared_deviations.mean()\n",
    "    \n",
    "    # Calculate the \"custom deviation\" (similar to standard deviation but centered on the wild type score)\n",
    "    custom_deviation = np.sqrt(mean_squared_deviation)\n",
    "    \n",
    "    # Compute modified Z-scores using the custom deviation\n",
    "    data['Scaled_' + score_column] = deviations / custom_deviation\n",
    "\n",
    "    return data, wild_type_score\n",
    "\n",
    "# Define functions to process DMS sample files\n",
    "def process_DMS_sample(file_path, score_column, drop_columns, rename_columns=None):\n",
    "    \"\"\"\n",
    "    Processes a DMS sample file by scaling scores, classifying sequences, and dropping unnecessary columns.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        dms_sample_df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        dms_sample_df, cutoff_value = scale_DMS_scores(dms_sample_df, score_column)\n",
    "        dms_sample_df['State'] = dms_sample_df[score_column].apply(lambda x: 'Benign' if x >= cutoff_value else 'Pathogenic')\n",
    "        if rename_columns:\n",
    "            dms_sample_df = dms_sample_df.rename(columns=rename_columns)\n",
    "        dms_sample_df = dms_sample_df.drop(columns=drop_columns)\n",
    "        dms_sample_df = dms_sample_df.iloc[1:]  # Drop the row with the wild type sequence\n",
    "        return dms_sample_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_directory(directory_path, score_column, drop_columns, rename_columns=None):\n",
    "    \"\"\"\n",
    "    Processes all DMS sample files in a directory by scaling scores, classifying sequences, and dropping unnecessary columns.\n",
    "    \"\"\"\n",
    "\n",
    "    processed_samples = []\n",
    "    for filename in filter(lambda x: x.endswith('.csv'), os.listdir(directory_path)):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        processed_df = process_DMS_sample(file_path, score_column, drop_columns, rename_columns)\n",
    "        if processed_df is not None:\n",
    "            processed_samples.append(processed_df)\n",
    "    return processed_samples\n",
    "\n",
    "# Process directories and update lists\n",
    "modified_human_DMS_samples_with_subs_list = process_directory(\n",
    "    modified_subs_path, \n",
    "    'DMS_score', \n",
    "    drop_columns=['mutant', 'DMS_score_bin'])\n",
    "\n",
    "modified_human_DMS_samples_with_indels_list = process_directory(\n",
    "    modified_indels_path,\n",
    "    'DMS_score',\n",
    "    drop_columns=['DMS_score_bin'],\n",
    "    rename_columns={'mutant': 'mutated_sequence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the results as histograms so that to study the distribution of the scaled DMS scores for each study.\n",
    "def plot_histogram(df, index, output_directory, state_column='State', score_column='DMS_score', scaled_score_column='Scaled_DMS_score'):\n",
    "    plt.figure(figsize=(20, 6))\n",
    "\n",
    "    # Create a subplot for the non-scaled scores\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(data=df, x=score_column, hue=state_column, \n",
    "                 palette={\"Pathogenic\": \"red\", \"Benign\": \"green\"}, \n",
    "                 bins=50, alpha=0.6)\n",
    "    plt.title(f'Non-Scaled DMS Scores - Sample {index+1}')\n",
    "    plt.xlabel('DMS Score')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Create a subplot for the scaled scores\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(data=df, x=scaled_score_column, hue=state_column, \n",
    "                 palette={\"Pathogenic\": \"red\", \"Benign\": \"green\"}, \n",
    "                 bins=50, alpha=0.6)\n",
    "    plt.title(f'Scaled DMS Scores - Sample {index+1}')\n",
    "    plt.xlabel('Scaled DMS Score')\n",
    "\n",
    "    # Save the figure\n",
    "    Path(output_directory)\n",
    "    plt.savefig(output_directory/f'Histogram_Comparison_{index+1}.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Apply the function to every sample \n",
    "for i, df in enumerate(modified_human_DMS_samples_with_subs_list):\n",
    "    plot_histogram(df, i, subs_output_directory)\n",
    "\n",
    "for i, df in enumerate(modified_human_DMS_samples_with_indels_list):\n",
    "    plot_histogram(df, i, indels_output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to build the automaton for the nullpeptide categories.\n",
    "def build_nullpeptide_automaton(nullpeptides):\n",
    "    \"\"\"\n",
    "    Creates an Aho - Corasick Automaton datastructure to store the nullpeptides.\n",
    "    \"\"\"\n",
    "    \n",
    "    nullpeptide_automaton = ahocorasick.Automaton() # Empty Automaton list.\n",
    "    for nullpeptide in nullpeptides:\n",
    "        nullpeptide_automaton.add_word(nullpeptide, nullpeptide) # Iterates through all the nullpeptides and stores each one to the automaton datastructure.\n",
    "    nullpeptide_automaton.make_automaton()\n",
    "    return nullpeptide_automaton\n",
    "\n",
    "def read_and_build_automatons(nullpeptides_path, nullpeptide_lengths):\n",
    "    \"\"\"\n",
    "    Reads nullpeptides of specific lengths and builds corresponding automatons.\n",
    "    \"\"\"\n",
    "\n",
    "    automatons = {} # Initialize an empty dictionary to store automatons.\n",
    "\n",
    "    # Loop through each specified nullpeptide length.\n",
    "    for nullpeptide_length in nullpeptide_lengths:\n",
    "        try:\n",
    "            file_path = f\"{nullpeptides_path}/nullpeptides_{nullpeptide_length}amino_acids_gencode.v43.pc_translations.txt\"\n",
    "            \n",
    "            # Read the nullpeptide sequences from the file into a pandas DataFrame.\n",
    "            nullpeptides_df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Convert the 'nullpeptides' column of the DataFrame into a list.\n",
    "            nullpeptides = nullpeptides_df['nullpeptides'].tolist()\n",
    "            \n",
    "            # Build an automaton for the current list of nullpeptides.\n",
    "            automatons[f'automaton_{nullpeptide_length}mer'] = build_nullpeptide_automaton(nullpeptides)\n",
    "        \n",
    "        # Catch and report any errors that occur during the process.\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing nullpeptides of length {nullpeptide_length}: {e}\")\n",
    "\n",
    "    return automatons\n",
    "\n",
    "nullpeptide_lengths = [4, 5, 6] # Specify nullpeptide length.\n",
    "\n",
    "# Create the nullpeptides automatons.\n",
    "automatons = read_and_build_automatons(nullpeptides_path, nullpeptide_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to search the presence of each nullpeptide in the mutated sequences of the DMS experiments.\n",
    "def find_nullpeptides(sequence, nullpeptide_automaton):\n",
    "    \"\"\"\n",
    "    Searches each mutated sequence from the DMS experiments for any occurences of the nullpeptides using the previously created Automaton.\n",
    "    \"\"\"\n",
    "    \n",
    "    nullpeptide_matches = []  # Initialize an empty list to store match information\n",
    "    for end_index, pattern in nullpeptide_automaton.iter(sequence):  \n",
    "        # Iterate over matches found by the automaton \n",
    "        start_index = end_index - len(pattern) + 1  # Calculate start of match\n",
    "        nullpeptide_matches.append((start_index, end_index, pattern)) # Store match details\n",
    "    return nullpeptide_matches  # Return the list of matches\n",
    "\n",
    "def process_DMS_sample(DMS_sample, nullpeptide_automaton, automaton_key):\n",
    "    \"\"\"\n",
    "    Searches DMS samples for present nullpeptides in mutated sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine the nullpeptide length from the automaton key\n",
    "    length = automaton_key.split('_')[1]\n",
    "\n",
    "    # Column names for nullpeptides and their counts\n",
    "    found_nullpeptides_col = f'{length}_Nullpeptides'\n",
    "    found_nullpeptides_counts_col = f'{length}_Nullpeptides_Counts'\n",
    "\n",
    "    # Initialize columns in DataFrame\n",
    "    DMS_sample[found_nullpeptides_col] = ''\n",
    "    DMS_sample[found_nullpeptides_counts_col] = 0\n",
    "\n",
    "    # Process each row to find nullpeptides and count them\n",
    "    for index, row in tqdm(DMS_sample.iterrows(), total=len(DMS_sample), desc=f\"Processing for {length}\"):\n",
    "        matches = find_nullpeptides(row['mutated_sequence'], nullpeptide_automaton)\n",
    "        if matches:\n",
    "            matched_nullpeptides = [match[2] for match in matches]\n",
    "            DMS_sample.at[index, found_nullpeptides_col] = ', '.join(matched_nullpeptides)\n",
    "            DMS_sample.at[index, found_nullpeptides_counts_col] = len(matches)\n",
    "\n",
    "    return DMS_sample\n",
    "\n",
    "def thorough_nullpeptide_search(samples_list, automatons):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame for each DMS experiment category and searches for nullpeptides.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Concatenate and sort the samples DataFrame\n",
    "    total_samples_df = pd.concat(samples_list, ignore_index=True)\n",
    "    total_samples_df.sort_values(by=['State', 'DMS_score'], ascending=[False, False], inplace=True)\n",
    "\n",
    "    # Process for each nullpeptide category defined by automatons\n",
    "    for key, automaton in automatons.items():\n",
    "        total_samples_df = process_DMS_sample(total_samples_df, automaton, key)\n",
    "\n",
    "    # Combine found nullpeptides into a single column and sum their counts\n",
    "    nullpeptide_cols = [f'{key.split(\"_\")[1]}_Nullpeptides' for key in automatons.keys()]\n",
    "    total_samples_df['Nullpeptides'] = total_samples_df[nullpeptide_cols].apply(lambda row: ', '.join(filter(None, row)), axis=1).str.strip(', ')\n",
    "\n",
    "    count_cols = [f'{key.split(\"_\")[1]}_Nullpeptides_Counts' for key in automatons.keys()]\n",
    "    total_samples_df['Nullpeptide_Counts'] = total_samples_df[count_cols].sum(axis=1)\n",
    "\n",
    "    return total_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the DMS Samples\n",
    "human_subs_nullp = thorough_nullpeptide_search(modified_human_DMS_samples_with_subs_list, automatons) # Substitutions\n",
    "human_indels_nullp = thorough_nullpeptide_search(modified_human_DMS_samples_with_indels_list, automatons) # Indels\n",
    "\n",
    "# Concatanate all results into a single DataFrame\n",
    "human_total_nullp = pd.concat([human_subs_nullp, human_indels_nullp]).drop(columns='DMS_score')\n",
    "human_total_nullp.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nullpeptide_median_scores_combined(df, output_file):\n",
    "    \"\"\"\n",
    "    Plots the median scaled DMS scores for each nullpeptide count for pathogenic and benign states,\n",
    "    combined into one big plot with subplots for each study type and nullpeptide length.\n",
    "    \"\"\"\n",
    "    study_types = df['Study_Type'].unique()\n",
    "    nullpeptide_lengths = [4, 5, 6]\n",
    "    states = ['Pathogenic', 'Benign']\n",
    "    state_colors = {'Pathogenic': 'red', 'Benign': 'green'}\n",
    "\n",
    "    # Create a single figure for all subplots\n",
    "    fig, axes = plt.subplots(len(study_types), len(nullpeptide_lengths), figsize=(18, 6 * len(study_types)), sharey='row')\n",
    "\n",
    "    if len(study_types) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for row, study_type in enumerate(study_types):\n",
    "        for col, length in enumerate(nullpeptide_lengths):\n",
    "            ax = axes[row][col] if len(study_types) > 1 else axes[col]\n",
    "            column_name = f'{length}mer_Nullpeptides'\n",
    "            \n",
    "            # Filter the DataFrame to keep rows with the specified nullpeptide length and rows with zero nullpeptides\n",
    "            df_length_filtered = df[(df[column_name].notna() & (df[column_name] != '')) | (df['Nullpeptide_Counts'] == 0)]\n",
    "            \n",
    "            for state in states:\n",
    "                state_df = df_length_filtered[(df_length_filtered['Study_Type'] == study_type) & (df_length_filtered['State'] == state)]\n",
    "                median_scores = state_df.groupby('Nullpeptide_Counts')['Scaled_DMS_score'].median().reset_index()\n",
    "\n",
    "                if not median_scores.empty:\n",
    "                    sns.lineplot(x='Nullpeptide_Counts', y='Scaled_DMS_score', data=median_scores, ax=ax, marker='o', linestyle='-', color=state_colors[state], label=state)\n",
    "            \n",
    "            ax.set_xlabel('Number of Nullpeptides')\n",
    "            ax.set_ylabel('Median DMS Score')\n",
    "            if col == 1:\n",
    "                ax.set_title(f'{study_type}\\n\\n{length}mer Nullpeptides')\n",
    "            else:\n",
    "                ax.set_title(f'{length}mer Nullpeptides')\n",
    "            \n",
    "            ax.axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "            ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "plot_nullpeptide_median_scores_combined(human_subs_nullp, plots_directory/\"subs_nullpeptide_number_score_correlation.png\")\n",
    "plot_nullpeptide_median_scores_combined(human_indels_nullp, plots_directory/\"indels_nullpeptide_number_score_correlation.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nullpeptide_comparison_by_study_type(df, output_file):\n",
    "    \"\"\"\n",
    "    Plots a comparison of nullpeptide scaled scores found in the DMS samples pre study type.\n",
    "    \"\"\"\n",
    "    study_types = df['Study_Type'].unique()\n",
    "    nullpeptide_lengths = [4, 5, 6]\n",
    "\n",
    "    group_palette = {\n",
    "        'Pathogenic with Nullpeptides': '#1f77b4',  # Blue\n",
    "        'Pathogenic without Nullpeptides': '#ff7f0e',  # Orange\n",
    "        'Benign with Nullpeptides': '#2ca02c',  # Green\n",
    "        'Benign without Nullpeptides': '#d62728'  # Red\n",
    "    }\n",
    "    # Create a single figure for all subplots\n",
    "    fig, axes = plt.subplots(len(study_types), len(nullpeptide_lengths), figsize=(18, 6 * len(study_types)), sharey='row')\n",
    "\n",
    "    if len(study_types) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for row, study_type in enumerate(study_types):\n",
    "        study_df = df[df['Study_Type'] == study_type]\n",
    "\n",
    "        for col, length in enumerate(nullpeptide_lengths):\n",
    "            ax = axes[row][col] if len(study_types) > 1 else axes[col]\n",
    "            column_name = f'{length}mer_Nullpeptides'\n",
    "\n",
    "            # Separate the data by pathogenic and benign states\n",
    "            pathogenic = study_df[study_df['State'] == 'Pathogenic']\n",
    "            benign = study_df[study_df['State'] == 'Benign']\n",
    "\n",
    "            # Further separate by presence of nullpeptides\n",
    "            pathogenic_with_nullpeptides = pathogenic[pathogenic[column_name].notna() & (pathogenic[column_name] != '')]\n",
    "            pathogenic_without_nullpeptides = pathogenic[pathogenic[column_name].isna() | (pathogenic[column_name] == '')]\n",
    "            benign_with_nullpeptides = benign[benign[column_name].notna() & (benign[column_name] != '')]\n",
    "            benign_without_nullpeptides = benign[benign[column_name].isna() | (benign[column_name] == '')]\n",
    "\n",
    "            # Prepare data for plotting\n",
    "            plot_data = pd.DataFrame({\n",
    "                'Scaled_DMS_score': pd.concat([\n",
    "                    pathogenic_with_nullpeptides['Scaled_DMS_score'], \n",
    "                    pathogenic_without_nullpeptides['Scaled_DMS_score'], \n",
    "                    benign_with_nullpeptides['Scaled_DMS_score'], \n",
    "                    benign_without_nullpeptides['Scaled_DMS_score']\n",
    "                ]),\n",
    "                'Group': (['Pathogenic with Nullpeptides'] * len(pathogenic_with_nullpeptides) + \n",
    "                          ['Pathogenic without Nullpeptides'] * len(pathogenic_without_nullpeptides) + \n",
    "                          ['Benign with Nullpeptides'] * len(benign_with_nullpeptides) + \n",
    "                          ['Benign without Nullpeptides'] * len(benign_without_nullpeptides))\n",
    "            })\n",
    "\n",
    "            if not plot_data.empty:\n",
    "                # Plot the scores using seaborn\n",
    "                sns.boxplot(x='Group', y='Scaled_DMS_score', data=plot_data, ax=ax, palette=group_palette, hue='Group')\n",
    "                sns.stripplot(x='Group', y='Scaled_DMS_score', data=plot_data, ax=ax, color='black', alpha=0.5, jitter=True)\n",
    "                ax.set_xlabel(' ')\n",
    "                ax.set_ylabel('Scaled DMS Score')\n",
    "                \n",
    "                if col == 1:\n",
    "                    ax.set_title(f'{study_type}\\n\\n{length}mer Nullpeptides')\n",
    "                else:\n",
    "                    ax.set_title(f'{length}mer Nullpeptides')\n",
    "\n",
    "                # Set ticks and labels\n",
    "                ax.set_xticks(range(len(plot_data['Group'].unique())))\n",
    "                ax.set_xticklabels(plot_data['Group'].unique(), rotation=45, size = 14)\n",
    "            else:\n",
    "                if col == 1:\n",
    "                    ax.set_title(f'{study_type}\\n{length}mer Nullpeptides')\n",
    "                else:\n",
    "                    ax.set_title(f'{length}mer Nullpeptides')\n",
    "                ax.set_xlabel(' ')\n",
    "                ax.set_ylabel('Scaled DMS Score')\n",
    "                ax.text(0.5, 0.5, 'No Data', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "plot_nullpeptide_comparison_by_study_type(human_subs_nullp, plots_directory/\"subs_scores_per_study_type.png\")\n",
    "plot_nullpeptide_comparison_by_study_type(human_indels_nullp, plots_directory/\"indels_scores_per_study_type.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the results as histograms so that to study the distribution of the scaled DMS scores\n",
    "def plot_distributions(df_subs, df_indels, figure_path):\n",
    "    \"\"\"\n",
    "    Plots the scaled DMS score distributions for substitutions and indels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plotting setup\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "    # Histogram for substitutions\n",
    "    sns.histplot(data=df_subs, x=\"Scaled_DMS_score\", hue=\"State\", palette={\"Pathogenic\": \"red\", \"Benign\": \"green\"}, bins=50, ax=axs[0], legend= False)\n",
    "    axs[0].set_title(\"Distribution of Scaled DMS Scores for Substitutions\", size=14)\n",
    "    axs[0].set_xlabel(\"Scaled DMS Score\", size=14)\n",
    "    axs[0].set_ylabel(\"Frequency\", size=14)\n",
    "    axs[0].axvline(x=0, color='k', linestyle='--')  # Vertical line at x=0\n",
    "    axs[0].tick_params(axis='both', which='major', labelsize=11)\n",
    "\n",
    "    # Histogram for indels\n",
    "    sns.histplot(data=df_indels, x=\"Scaled_DMS_score\", hue=\"State\", palette={\"Pathogenic\": \"red\", \"Benign\": \"green\"}, bins=50, ax=axs[1])\n",
    "    axs[1].set_title(\"Distribution of Scaled DMS Scores for Indels\", size=14)\n",
    "    axs[1].set_xlabel(\"Scaled DMS Score\", size=14)\n",
    "    axs[1].set_ylabel(\" \")\n",
    "    axs[1].axvline(x=0, color='k', linestyle='--')  # Vertical line at x=0\n",
    "    axs[1].tick_params(axis='both', which='major', labelsize=11)  # Adjust labelsize to your preference\n",
    "    \n",
    "    # Layout adjustment and saving\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    plt.savefig(figure_path, dpi = 300)\n",
    "    plt.show()\n",
    "\n",
    "# Plot the distributions of the scaled DMS scores for substitutions and indels.\n",
    "plot_distributions(human_subs_nullp, human_indels_nullp, plots_directory/\"DMS_scores_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the results as barplots\n",
    "def barplot_nullpeptides(subs_df, indels_df, nullpeptide_column, output_filename):\n",
    "    \"\"\"\n",
    "    Creates a barplot of all 6-mer nullpeptides found in substitution and indel samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a figure with two subplots side by side.\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)  # Share y-axis for better comparison\n",
    "    \n",
    "    # Prepare datasets for both substitutions and indels\n",
    "    datasets = {'Substitutions': subs_df, 'Indels': indels_df}\n",
    "\n",
    "    for idx, (label, DMS_df) in enumerate(datasets.items()):\n",
    "        # Count total sequences for each state in the current dataset\n",
    "        total_pathogenic_count = DMS_df[DMS_df['State'] == 'Pathogenic']['mutated_sequence'].count()\n",
    "        total_benign_count = DMS_df[DMS_df['State'] == 'Benign']['mutated_sequence'].count()\n",
    "\n",
    "        # Count sequences with at least one nullpeptide present in each state\n",
    "        pathogenic_with_nullpeptide_count = DMS_df[(DMS_df['State'] == 'Pathogenic') & (DMS_df[nullpeptide_column] > 0)]['mutated_sequence'].count()\n",
    "        benign_with_nullpeptide_count = DMS_df[(DMS_df['State'] == 'Benign') & (DMS_df[nullpeptide_column] > 0)]['mutated_sequence'].count()\n",
    "\n",
    "        # Calculate proportion of counts with nullpeptides compared to total counts\n",
    "        proportion_pathogenic_with_nullpeptide = pathogenic_with_nullpeptide_count / total_pathogenic_count\n",
    "        proportion_benign_with_nullpeptide = benign_with_nullpeptide_count / total_benign_count\n",
    "\n",
    "        # Create a DataFrame for plotting\n",
    "        plot_data = pd.DataFrame({\n",
    "            'State': ['Benign', 'Pathogenic'],\n",
    "            'Proportion with Nullpeptides': [proportion_benign_with_nullpeptide, proportion_pathogenic_with_nullpeptide]\n",
    "        })\n",
    "\n",
    "        # Use seaborn to plot the bar plot on the appropriate subplot\n",
    "        sns.barplot(x='State', y='Proportion with Nullpeptides', hue='State', data=plot_data, palette=['blue', 'red'], ax=axes[idx])\n",
    "        axes[idx].set_title(f'{label}', fontsize=14)\n",
    "        axes[idx].set_xlabel('')  # Remove X-axis label\n",
    "\n",
    "        # Set Y-axis label only for the left subplot\n",
    "        if idx == 0:\n",
    "            axes[idx].set_ylabel('Proportion with Nullpeptides', fontsize=12)\n",
    "        else:\n",
    "            axes[idx].set_ylabel('')\n",
    "\n",
    "        axes[idx].set_ylim(0, 1)  # Set Y-axis limits to 0 and 1\n",
    "        axes[idx].grid(True, linestyle='-', alpha=0.7)  # Add grid lines\n",
    "\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    plt.savefig(output_filename, dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Create a barplot of all 6-mer nullpeptides found in substitution and indel samples\n",
    "barplot_nullpeptides(human_subs_nullp, human_indels_nullp, '6mer_Nullpeptides_Counts', plots_directory/\"subs_indels_comparison_barplots.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot showcasing the proportions of nullpeptide counts in substitutions and indel mutations\n",
    "def plot_nullpeptides_proportions(subs_df, indels_df, output_filepath):\n",
    "    \"\"\"\n",
    "    Processes the DataFrames obtained from the Aho-Corasick algorithm for substitutions and indels,\n",
    "    and plots the results as barplots that showcase the number of 6mer nullpeptides present in each state.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "    \n",
    "    # DataFrames to process: one for substitutions, one for indels\n",
    "    datasets = {'Substitutions': subs_df, 'Indels': indels_df}\n",
    "    column_name = '6mer_Nullpeptides_Counts'\n",
    "    max_count = 5  # Define the maximum count for binning the nullpeptide counts\n",
    "\n",
    "    for idx, (label, dataframe) in enumerate(datasets.items()):\n",
    "        pathogenic = dataframe[dataframe['State'] == 'Pathogenic'][column_name]\n",
    "        benign = dataframe[dataframe['State'] == 'Benign'][column_name]\n",
    "        \n",
    "        # Create bins with the last bin grouping all counts >= max_count\n",
    "        bins = list(range(0, max_count)) + [max_count, max(pathogenic.max(), benign.max()) + 1]\n",
    "\n",
    "        # Calculate histogram data\n",
    "        hist_pathogenic, _ = np.histogram(pathogenic, bins=bins)\n",
    "        hist_benign, bin_edges = np.histogram(benign, bins=bins)\n",
    "        \n",
    "        # Calculate proportions\n",
    "        proportions_pathogenic = hist_pathogenic / hist_pathogenic.sum()\n",
    "        proportions_benign = hist_benign / hist_benign.sum()\n",
    "\n",
    "        # Create bin labels with the last label for 'max_count or more'\n",
    "        bin_labels = [f\"{i}\" for i in bin_edges[:-2]] + [f\"{max_count} or more\"]\n",
    "        \n",
    "        # DataFrame for plotting\n",
    "        plot_data = pd.DataFrame({\n",
    "            'Number of Nullpeptides': bin_labels * 2,\n",
    "            'Proportion': np.concatenate([proportions_pathogenic, proportions_benign]),\n",
    "            'Condition': ['Pathogenic'] * len(bin_labels) + ['Benign'] * len(bin_labels)\n",
    "        })\n",
    "\n",
    "        # Plot using seaborn on the appropriate axis\n",
    "        sns.barplot(x='Number of Nullpeptides', y='Proportion', hue='Condition', data=plot_data, ax=axes[idx], palette='viridis', alpha=0.8)\n",
    "        axes[idx].set_title(f'{label}', fontsize=12)\n",
    "        axes[idx].set_xlabel('Number of 6mer Nullpeptides', fontsize= 12)\n",
    "        axes[idx].set_ylim(0, 1)\n",
    "        axes[idx].set_ylabel('Proportion' if idx == 0 else '', fontsize= 12)\n",
    "        tick_positions = np.arange(len(bin_labels))\n",
    "        axes[idx].set_xticks(tick_positions)\n",
    "        axes[idx].set_xticklabels(bin_labels, rotation=35)\n",
    "        axes[idx].grid(True, linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Display legend only on the rightmost plot\n",
    "        if idx == len(axes) - 1:\n",
    "            axes[idx].legend()\n",
    "        else:\n",
    "            axes[idx].legend([],[], frameon=False)  # Hide legend for other plots\n",
    "\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    plt.savefig(output_filepath, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Plot the proportions of 6mer nullpeptides found in substitution and indel samples\n",
    "plot_nullpeptides_proportions(\n",
    "    subs_df=human_subs_nullp, \n",
    "    indels_df=human_indels_nullp, \n",
    "    output_filepath=plots_directory/\"subs_indels_binned_nullpeptides.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to filter rows with 0 nullpeptides and also change types to improve memmory efficiency.\n",
    "def filter_and_optimize_DMS_dataframe(DMS_df, remove_null_rows=False):\n",
    "    \"\"\"\n",
    "    Filters and optimizes the DMS DataFrame for memory usage and processing speed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter the DataFrame to keep only rows with Nullpeptide_Counts greater than 0\n",
    "    if remove_null_rows == True:\n",
    "        filtered_df = DMS_df[DMS_df['Nullpeptide_Counts'] > 0]\n",
    "    elif remove_null_rows == False:\n",
    "        filtered_df = DMS_df.copy()\n",
    "    filtered_df = filtered_df.drop(columns=['mutated_sequence', 'DMS_score', '4mer_Nullpeptides', '4mer_Nullpeptides_Counts', '5mer_Nullpeptides', '5mer_Nullpeptides_Counts', 'Nullpeptides', 'Nullpeptide_Counts'])\n",
    "    filtered_df = filtered_df.rename(columns={\"6mer_Nullpeptides\": \"Nullpeptides\", \"6mer_Nullpeptides_Counts\": \"Nullpeptide_Counts\"})\n",
    "\n",
    "    # Convert 'State' to categorical data type\n",
    "    filtered_df['State'] = filtered_df['State'].astype('category')\n",
    "\n",
    "    # Convert 'Sqrt_DMS_score' to float32\n",
    "    filtered_df['Scaled_DMS_score'] = filtered_df['Scaled_DMS_score'].astype('float32')\n",
    "\n",
    "    # Convert 'Nullpeptides' to string\n",
    "    filtered_df['Nullpeptides'] = filtered_df['Nullpeptides'].astype('str')\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# Apply the function and print the info for each filtered dataframe\n",
    "filtered_subs_df = filter_and_optimize_DMS_dataframe(human_subs_nullp, remove_null_rows=False)\n",
    "print('Substitutions dataframe info:', filtered_subs_df.info())\n",
    "print(\" \")\n",
    "\n",
    "filtered_indels_df = filter_and_optimize_DMS_dataframe(human_indels_nullp, remove_null_rows=False)\n",
    "print('Indels dataframe info:', filtered_indels_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to encode nullpeptides in a format with counts for each state and total counts or in a format with scores for each state and total counts\n",
    "def nullpeptide_encoding(ahocorasick_df, peptides_column, state_column, score_column=None, fisher_exact_results=None, encoding='count'):\n",
    "    \"\"\"\n",
    "    Encode each nullpeptide in a format with counts for each state and total counts, or encode based on scores if specified.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy dataframe to avoid modifying the original and split peptides into individual rows\n",
    "    exploded_ahocorasick_df = ahocorasick_df.copy()\n",
    "    exploded_ahocorasick_df[peptides_column] = exploded_ahocorasick_df[peptides_column].str.split(', ')\n",
    "    exploded_ahocorasick_df = exploded_ahocorasick_df.explode(peptides_column)\n",
    "    \n",
    "    # Count Pathogenic and Benign occurrences for each nullpeptide\n",
    "    exploded_ahocorasick_df['Pathogenic_counts'] = (exploded_ahocorasick_df[state_column] == 'Pathogenic').astype(int)\n",
    "    exploded_ahocorasick_df['Benign_counts'] = (exploded_ahocorasick_df[state_column] == 'Benign').astype(int)\n",
    "\n",
    "    if encoding == 'count':\n",
    "        # Aggregate the counts for each nullpeptide\n",
    "        aggregated_ahocorasick_df = exploded_ahocorasick_df.groupby(peptides_column, as_index=False).agg({\n",
    "            'Pathogenic_counts': 'sum',\n",
    "            'Benign_counts': 'sum'\n",
    "        })\n",
    "\n",
    "        aggregated_ahocorasick_df['Total_counts'] = aggregated_ahocorasick_df['Pathogenic_counts'] + aggregated_ahocorasick_df['Benign_counts']\n",
    "        aggregated_ahocorasick_df = aggregated_ahocorasick_df.sort_values(by='Total_counts', ascending=False)\n",
    "        aggregated_ahocorasick_df.reset_index(drop=True, inplace=True)\n",
    "        aggregated_ahocorasick_df = aggregated_ahocorasick_df.reindex(columns=['Nullpeptides', 'Pathogenic_counts', 'Benign_counts', 'Total_counts'])\n",
    "\n",
    "        return aggregated_ahocorasick_df\n",
    "\n",
    "    elif encoding == 'score':\n",
    "        # Initialize the score columns and aggregate scores along with counts\n",
    "        exploded_ahocorasick_df['Pathogenic_scores'] = exploded_ahocorasick_df.apply(\n",
    "            lambda x: [x[score_column]] if x[state_column] == 'Pathogenic' else [], axis=1)\n",
    "        exploded_ahocorasick_df['Benign_scores'] = exploded_ahocorasick_df.apply(\n",
    "            lambda x: [x[score_column]] if x[state_column] == 'Benign' else [], axis=1)\n",
    "\n",
    "        aggregated_ahocorasick_df = exploded_ahocorasick_df.groupby(peptides_column, as_index=False).agg({\n",
    "            'Pathogenic_counts': 'sum',\n",
    "            'Benign_counts': 'sum',\n",
    "            'Pathogenic_scores': lambda x: np.array([item for sublist in x for item in sublist] if x.tolist() else [0]),\n",
    "            'Benign_scores': lambda x: np.array([item for sublist in x for item in sublist] if x.tolist() else [0])\n",
    "        })\n",
    "        aggregated_ahocorasick_df['Total_counts'] = aggregated_ahocorasick_df['Pathogenic_counts'] + aggregated_ahocorasick_df['Benign_counts']\n",
    "        aggregated_ahocorasick_df['Total_counts'] = aggregated_ahocorasick_df['Total_counts'].astype(np.int16)\n",
    "        aggregated_ahocorasick_df = aggregated_ahocorasick_df.reindex(columns=['Nullpeptides', 'Pathogenic_counts', 'Benign_counts', 'Total_counts', 'Pathogenic_scores', 'Benign_scores'])\n",
    "\n",
    "        # Merge with Fisher's exact test results\n",
    "        merged_ahocorasick_df = pd.merge(aggregated_ahocorasick_df, fisher_exact_results, how='left', left_on=peptides_column, right_on='Significant_Nullpeptides')\n",
    "        merged_ahocorasick_df[['Odds_Ratio', 'Fisher_P_Value']] = merged_ahocorasick_df[['Odds_Ratio', 'Fisher_P_Value']].astype(np.float32)\n",
    "\n",
    "        # Apply the lambda function to handle empty lists explicitly\n",
    "        merged_ahocorasick_df['Benign_scores'] = merged_ahocorasick_df['Benign_scores'].apply(lambda x: [0] if len(x) == 0 else x)\n",
    "        merged_ahocorasick_df['Pathogenic_scores'] = merged_ahocorasick_df['Pathogenic_scores'].apply(lambda x: [0] if len(x) == 0 else x)\n",
    "\n",
    "        merged_ahocorasick_df = merged_ahocorasick_df.sort_values(by='Odds_Ratio', ascending=False)\n",
    "        return merged_ahocorasick_df\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid encoding type specified. Choose 'count' or 'score'.\")\n",
    "    \n",
    "\n",
    "# Perform statistical testing to detect nullpeptides associated with each state. Fisher exact test is used because for some nullpeptides sample size is below 5 counts.\n",
    "def perform_fisher_exact_test(encoded_nullpeptides_df, test_method):\n",
    "    \"\"\"\n",
    "    Perform Fisher Exact Test on a dataset and optionally apply a multiple testing correction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    encoded_nullpeptides_df_copy = encoded_nullpeptides_df.copy()\n",
    "\n",
    "    # Convert integer counts to int16 data type for more efficient data handling\n",
    "    encoded_nullpeptides_df_copy = encoded_nullpeptides_df_copy.astype({'Pathogenic_counts': 'int16', 'Benign_counts': 'int16', 'Total_counts': 'int16'})\n",
    "\n",
    "    # Calculate total sums once\n",
    "    total_pathogenic_counts = encoded_nullpeptides_df_copy['Pathogenic_counts'].sum()\n",
    "    total_benign_counts = encoded_nullpeptides_df_copy['Benign_counts'].sum()\n",
    "\n",
    "    # Store results in a list of dictionaries\n",
    "    results = []\n",
    "\n",
    "    # Iterate over the DataFrame rows\n",
    "    for _, row in encoded_nullpeptides_df_copy.iterrows():\n",
    "        # Current nullpeptide counts\n",
    "        current_nullepeptide_pathogenic_counts = row['Pathogenic_counts'] + 1\n",
    "        current_nullepeptide_benign_counts = row['Benign_counts'] + 1\n",
    "        \n",
    "        # Counts for other nullpeptides (total minus current)\n",
    "        other_nullpeptides_pathogenic_counts = total_pathogenic_counts - current_nullepeptide_pathogenic_counts + 1\n",
    "        other_nullpeptides_benign_counts = total_benign_counts - current_nullepeptide_benign_counts + 1 \n",
    "        \n",
    "        # Construct the contingency table of this format\n",
    "        \"\"\"\n",
    "        |             | Pathogenic | Benign |\n",
    "        |-------------|------------|--------|\n",
    "        | Nullpeptide |      x     |    y   |\n",
    "        | Other       |      a     |    b   |\n",
    "        \"\"\"\n",
    "        contingency_table = np.array([[current_nullepeptide_pathogenic_counts , current_nullepeptide_benign_counts], \n",
    "                                      [other_nullpeptides_pathogenic_counts, other_nullpeptides_benign_counts]])\n",
    "        \n",
    "        # Perform the Fisher Exact Test\n",
    "        # The test will assess whether the odds of experiencing a specific nullpeptide being pathogenic are significantly greater compared to the rest of the nullpeptides.\n",
    "        odds_ratio, p_value = stats.fisher_exact(contingency_table, test_method)\n",
    "\n",
    "        # Append the results\n",
    "        results.append({'Significant_Nullpeptides': row['Nullpeptides'], \n",
    "                        'Odds_Ratio': odds_ratio, \n",
    "                        'Fisher_P_Value': p_value})\n",
    "\n",
    "    # Convert results to a DataFrame for easier viewing and analysis\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Keep only nullpeptides with adjusted p-value less than 0.05\n",
    "    results_df = results_df[results_df['Fisher_P_Value'] <= 0.05]\n",
    "\n",
    "    # Sort nullepeptides based on adjusted p-value\n",
    "    results_df = results_df.sort_values(by=['Odds_Ratio', 'Fisher_P_Value'], ascending=[False, True])\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Define a function to filter the substitutions and indels dataframes to keep only the stastistically significant nullpeptides\n",
    "def extract_nullpeptides(nullpeptide_dataframe, test_results, nullpeptide_column, fisher_test_results=True):\n",
    "    \"\"\"\n",
    "    Filters nullpeptides in a DataFrame based on the results of the Fisher's exact test.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the unique nullpeptides from the test results\n",
    "    unique_nullpeptides = set(test_results[nullpeptide_column])\n",
    "\n",
    "    # Helper function to process each row\n",
    "    def filter_nullpeptides(peptides):\n",
    "        if peptides is None:\n",
    "            return None\n",
    "        peptides_list = peptides.split(', ')\n",
    "        filtered_significant_peptides = ', '.join([peptide for peptide in peptides_list if peptide in unique_nullpeptides])\n",
    "        return filtered_significant_peptides if filtered_significant_peptides else None\n",
    "\n",
    "    # Apply the helper function and assign to a new column\n",
    "    nullpeptide_dataframe['Nullpeptides'] = nullpeptide_dataframe['Nullpeptides'].apply(filter_nullpeptides)\n",
    "\n",
    "    # Drop rows where 'Significant_Nullpeptides' is None (i.e., no significant peptides) and drop unused original columns\n",
    "    # nullpeptide_dataframe.dropna(subset=['Nullpeptides'], inplace=True)\n",
    "\n",
    "    if fisher_test_results == True:\n",
    "        nullpeptide_dataframe = nullpeptide_dataframe.drop(columns=['Nullpeptide_Counts'])\n",
    "        \n",
    "    return nullpeptide_dataframe\n",
    "\n",
    "# Define a function to perform the Mann-Whitney U test to further filter nullpeptides based on scores.\n",
    "def perform_mann_whitney_test(score_encoded_nullpeptides):\n",
    "    \"\"\"\n",
    "    Applies the Mann-Whitney U test to compare 'Pathogenic_scores' and 'Benign_scores' for each row in the dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize lists to store test results\n",
    "    u_stats = []\n",
    "    p_values = []\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for _, row in score_encoded_nullpeptides.iterrows():\n",
    "        # Extract scores for pathogenic and benign\n",
    "        pathogenic_scores = row['Pathogenic_scores']\n",
    "        benign_scores = row['Benign_scores']\n",
    "        \n",
    "        # Perform Mann-Whitney U test\n",
    "        u_stat, p_value = stats.mannwhitneyu(pathogenic_scores, benign_scores, alternative='two-sided')\n",
    "        \n",
    "        # Append results\n",
    "        u_stats.append(u_stat)\n",
    "        p_values.append(p_value)\n",
    "    \n",
    "    # Add a column containing Mann-Whitney U test p-value results\n",
    "    score_encoded_nullpeptides['Mann_Whitney_P_Value'] = p_values\n",
    "\n",
    "    # Filter results based on significance level\n",
    "    filtered_score_encoded_nullpeptides = score_encoded_nullpeptides[score_encoded_nullpeptides['Mann_Whitney_P_Value'] <= 0.05]\n",
    "    \n",
    "    return filtered_score_encoded_nullpeptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process Aho-Corasick results using previously defined functions.\n",
    "def detect_significant_nullpeptides_by_protein(original_dataframe):\n",
    "    \"\"\"\n",
    "    Processes a DataFrame by splitting it by 'Protein', encoding nullpeptides, performing Fisher's exact test, and filtering significant nullpeptides.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the DataFrame into smaller DataFrames for each unique protein\n",
    "    protein_dataframes = {protein: group for protein, group in original_dataframe.groupby('Protein')}\n",
    "\n",
    "    # Initialize a list to collect results DataFrames\n",
    "    significant_nullpeptides_dfs = []\n",
    "    mannwhitney_dfs = []\n",
    "\n",
    "    # Process each smaller DataFrame\n",
    "    for protein, data in tqdm(protein_dataframes.items(), desc=\"Processing individual proteins\"):\n",
    "        \n",
    "        # Encode nullpeptides based on State counts for each split DataFrame\n",
    "        count_encoded_df = nullpeptide_encoding(data, 'Nullpeptides', 'State', encoding='count')\n",
    "        \n",
    "        # Perform statistical testing on the encoded data\n",
    "        fisher_results = perform_fisher_exact_test(count_encoded_df, test_method='two-sided')\n",
    "        \n",
    "        # Filter the original data to keep only significant nullpeptides\n",
    "        fisher_significant_df = extract_nullpeptides(data, fisher_results, 'Significant_Nullpeptides', fisher_test_results=True)\n",
    "\n",
    "        # Encode each significant nullpeptide based on the results of the Fisher's exact test, it's score and it's counts\n",
    "        score_encoded_df = nullpeptide_encoding(fisher_significant_df, 'Nullpeptides', 'State', 'Scaled_DMS_score', fisher_results, encoding='score')\n",
    "\n",
    "        # Perform Mann-Whitney U test\n",
    "        mannwhitney_results = perform_mann_whitney_test(score_encoded_df)\n",
    "\n",
    "        # Filter the dataframe to keep only significant nullpeptides found by the Mann-Whitney U test\n",
    "        mannwhitney_significant_df = extract_nullpeptides(fisher_significant_df, mannwhitney_results, 'Significant_Nullpeptides', fisher_test_results=False)\n",
    "\n",
    "        # Add a column to distinguish results from each protein\n",
    "        mannwhitney_results['Protein'] = protein\n",
    "        mannwhitney_significant_df['Protein'] = protein\n",
    "        \n",
    "        # Collect the processed DataFrame\n",
    "        mannwhitney_dfs.append(mannwhitney_results)\n",
    "        significant_nullpeptides_dfs.append(mannwhitney_significant_df)\n",
    "\n",
    "        # Concatanate into a final DataFrame\n",
    "        final_mannwhitney_results = pd.concat(mannwhitney_dfs)\n",
    "        final_significant_df = pd.concat(significant_nullpeptides_dfs)\n",
    "\n",
    "        # Reset the index\n",
    "        final_mannwhitney_results.reset_index(drop=True, inplace=True)\n",
    "        final_significant_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Drop unused columns\n",
    "        final_mannwhitney_results = final_mannwhitney_results.drop(columns=['Fisher_P_Value', 'Significant_Nullpeptides'])\n",
    "\n",
    "    # Concatenate all results into a single DataFrame\n",
    "    return final_mannwhitney_results, final_significant_df\n",
    "\n",
    "# Detect significant nullpeptides based on individual proteins for substitutions and indels\n",
    "score_encoded_nullpeptides_subs, significant_nullpeptides_subs = detect_significant_nullpeptides_by_protein(filtered_subs_df)\n",
    "score_encoded_nullpeptides_indels, significant_nullpeptides_indels = detect_significant_nullpeptides_by_protein(filtered_indels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_nullpeptides_subs = significant_nullpeptides_subs.dropna(subset=['Nullpeptides'])\n",
    "significant_nullpeptides_indels = significant_nullpeptides_indels.dropna(subset=['Nullpeptides'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply network analysis to find commonly co-occurring nullpeptides for each protein. Also, apply filters to keep those that are found the most times and have a significant score.\n",
    "def build_nullpeptide_network(significant_nullpeptide_df, output_file, min_edge_threshold=5):\n",
    "    \"\"\"\n",
    "    Constructs a network connecting nullpeptides that co-occur in the same DataFrame row,\n",
    "    labeling edges as 'Pathogenic' or 'Benign' based on their occurrence patterns, and\n",
    "    adds a score attribute for each edge.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty undirected graph\n",
    "    graph = nx.Graph()\n",
    "\n",
    "    # Process each record in the DataFrame\n",
    "    for _, record in significant_nullpeptide_df.iterrows():\n",
    "        peptides = [peptide.strip() for peptide in record['Nullpeptides'].split(\", \")]\n",
    "        state = record['State']\n",
    "        score = record['Scaled_DMS_score']\n",
    "\n",
    "        # Create edges between all pairs of peptides in the row\n",
    "        for i in range(len(peptides)):\n",
    "            for j in range(i + 1, len(peptides)):\n",
    "                peptide1, peptide2 = peptides[i], peptides[j]\n",
    "\n",
    "                if graph.has_edge(peptide1, peptide2):\n",
    "                    edge_data = graph[peptide1][peptide2]\n",
    "                    edge_data['Total_count'] += 1\n",
    "                    edge_data['Score_sum'] += score\n",
    "                    if state == 'Pathogenic':\n",
    "                        edge_data['Pathogenic_count'] += 1\n",
    "                    else:\n",
    "                        edge_data['Benign_count'] += 1\n",
    "                else:\n",
    "                    graph.add_edge(peptide1, peptide2, Total_count=1,\n",
    "                                   Score_sum=score,\n",
    "                                   Pathogenic_count=1 if state == 'Pathogenic' else 0,\n",
    "                                   Benign_count=1 if state != 'Pathogenic' else 0)\n",
    "\n",
    "    # Calculate mean score for each edge and remove those not meeting score threshold\n",
    "    edges_to_remove = []\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        mean_score = data['Score_sum'] / data['Total_count']\n",
    "        if abs(mean_score) < 1 or data['Total_count'] < min_edge_threshold:\n",
    "            edges_to_remove.append((u, v))\n",
    "        else:\n",
    "            data['Mean_score'] = mean_score\n",
    "\n",
    "    graph.remove_edges_from(edges_to_remove)\n",
    "\n",
    "    # Label remaining edges\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        data['Label'] = 'Pathogenic' if data['Pathogenic_count'] > data['Benign_count'] else 'Benign'\n",
    "\n",
    "    # Remove isolated nodes\n",
    "    isolated_nodes = list(nx.isolates(graph))\n",
    "    graph.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "    # Save the graph\n",
    "    nx.write_graphml(graph, output_file)\n",
    "\n",
    "    return graph\n",
    "\n",
    "def create_adjacency_matrix(graph):\n",
    "    \"\"\"\n",
    "    Creates and orders an adjacency matrix so that similar subnetworks are not across the diagonal.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract nodes and separate them by label\n",
    "    benign_nodes = [n for n, d in graph.nodes(data=True) if any(graph[u][v]['Label'] == 'Benign' for u, v in graph.edges(n))]\n",
    "    pathogenic_nodes = [n for n, d in graph.nodes(data=True) if any(graph[u][v]['Label'] == 'Pathogenic' for u, v in graph.edges(n))]\n",
    "\n",
    "    # Order nodes to emphasize grouping: benign nodes at the beginning and pathogenic at the end\n",
    "    ordered_nodes = benign_nodes + pathogenic_nodes\n",
    "\n",
    "    # Create an empty DataFrame based on the reordered nodes\n",
    "    adj_matrix = pd.DataFrame(data=0, index=ordered_nodes, columns=ordered_nodes)\n",
    "\n",
    "    # Fill the matrix with benign counts as positive and pathogenic counts as negative\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        value = data['Benign_count'] if data['Label'] == 'Benign' else -data['Pathogenic_count']\n",
    "        adj_matrix.at[u, v] = value\n",
    "        adj_matrix.at[v, u] = value  # Since the graph is undirected\n",
    "\n",
    "    return adj_matrix\n",
    "\n",
    "def find_top_peptides(adj_matrix, top_n=10):\n",
    "    \"\"\"\n",
    "    Finds the top 'n' peptides in the adjacency matrix based on the absolute interaction counts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the sum of absolute interactions for each peptide\n",
    "    absolute_sums = adj_matrix.abs().sum().sort_values(ascending=False)\n",
    "\n",
    "    # Get the top 'n' peptides based on absolute interaction counts\n",
    "    top_peptides = absolute_sums.head(top_n).reset_index()\n",
    "    top_peptides.columns = ['Peptide', 'Counts']\n",
    "\n",
    "    return top_peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_networks = {} # Dictionary to hold the networks for substitutions\n",
    "indels_networks = {} # Dictionary to hold the networks for indels\n",
    "\n",
    "subs_adjacency_matrices = {} # Dictionary to hold the adjacency matrices for substitutions\n",
    "indels_adjacency_matrices = {} # Dictionary to hold the adjacency matrices for indels\n",
    "\n",
    "subs_top_peptides = {} # Dictionary to hold the top occuring nullpeptides from substitutions for each protein \n",
    "indels_top_peptides = {} # Dictionary to hold the top occuring nullpeptides from indels for each protein\n",
    "\n",
    "# Helper function to filter the adjacency matrix based on a threshold for occurences a nullpeptide a must possess.\n",
    "def filter_adjacency_matrix(adj_matrix, threshold=100):\n",
    "    \"\"\"\n",
    "    Filters the adjacency matrix based on a threshold for the number of occurences a nullpeptide a must possess.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mask for rows where the max absolute value is greater than the threshold\n",
    "    mask = adj_matrix.map(abs).max(axis=1) > threshold\n",
    "    # Filter rows and columns based on the mask\n",
    "    filtered_matrix = adj_matrix.loc[mask, mask]\n",
    "\n",
    "    return filtered_matrix\n",
    "\n",
    "def process_nullpeptide_data(significant_nullpeptides, score_encoded_nullpeptides, mutation_type):\n",
    "    \"\"\"\n",
    "    Creates and filters a network and its corresponding adjacency matrix showing relationships between nullpeptides per protein. \n",
    "    Also, identifies the top 10 most commonly occuring nullpeptides for each protein.\n",
    "    \"\"\"\n",
    "\n",
    "    # Choose the correct dictionary for the mutation type\n",
    "    if mutation_type == 'subs':\n",
    "        networks_dict = subs_networks\n",
    "        adj_matrices_dict = subs_adjacency_matrices\n",
    "        top_peptides_dict = subs_top_peptides\n",
    "        min_edge_threshold = 5\n",
    "\n",
    "    elif mutation_type == 'indels':\n",
    "        networks_dict = indels_networks\n",
    "        adj_matrices_dict = indels_adjacency_matrices\n",
    "        top_peptides_dict = indels_top_peptides\n",
    "        min_edge_threshold = 1\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mutation type: {mutation_type}\")\n",
    "\n",
    "    # Iterate over each unique protein in the DataFrame\n",
    "    for protein in significant_nullpeptides['Protein'].unique():\n",
    "        protein_df = significant_nullpeptides[significant_nullpeptides['Protein'] == protein]\n",
    "        output_filename = f\"{networks_directory}/{protein}_{mutation_type}_network.graphml\"\n",
    "        graph = build_nullpeptide_network(protein_df, output_filename, min_edge_threshold=min_edge_threshold)\n",
    "\n",
    "        networks_dict[f\"{protein}_network\"] = graph\n",
    "\n",
    "        # Create adjacency matrix\n",
    "        adj_matrix = create_adjacency_matrix(graph)\n",
    "\n",
    "        # Special handling for specific proteins like 'RASK'\n",
    "        if protein == 'RASK':\n",
    "            adj_matrix = filter_adjacency_matrix(adj_matrix)  # Specific filtering for RASK\n",
    "\n",
    "        adj_matrices_dict[f\"{protein}_adj_matrix\"] = adj_matrix\n",
    "\n",
    "        # Find top peptides based on absolute counts\n",
    "        top_peptides_df = find_top_peptides(adj_matrix)\n",
    "        top_peptides_dict[protein] = top_peptides_df['Peptide'].tolist()\n",
    "\n",
    "    # Filter DataFrame to include only rows with top peptides\n",
    "    most_frequent_nullpeptides = []\n",
    "    score_encoded_nullpeptides_copy = score_encoded_nullpeptides.copy()\n",
    "\n",
    "    for protein, peptides in top_peptides_dict.items():\n",
    "        filtered_protein_df = score_encoded_nullpeptides_copy[\n",
    "            (score_encoded_nullpeptides_copy['Protein'] == protein) & \n",
    "            (score_encoded_nullpeptides_copy['Nullpeptides'].isin(peptides))\n",
    "        ]\n",
    "        most_frequent_nullpeptides.append(filtered_protein_df)\n",
    "\n",
    "    # Concatenate all filtered rows into a single DataFrame\n",
    "    filtered_df = pd.concat(most_frequent_nullpeptides, ignore_index=True)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "filtered_score_encoded_nullpeptides_subs = process_nullpeptide_data(significant_nullpeptides_subs, score_encoded_nullpeptides_subs, 'subs')\n",
    "filtered_score_encoded_nullpeptides_indels = process_nullpeptide_data(significant_nullpeptides_indels, score_encoded_nullpeptides_indels, \"indels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adjacency_matrices(adj_matrices, plot_filename, mutation_type, overall_title):\n",
    "    \"\"\"\n",
    "    Plots multiple adjacency matrices as subplots in a single figure, skipping empty matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter out empty or all-zero adjacency matrices\n",
    "    filtered_matrices = {k: v for k, v in adj_matrices.items() if not v.empty and not v.isnull().all().all() and not (v == 0).all().all()}\n",
    "\n",
    "    # Determine the number of subplots needed\n",
    "    n_matrices = len(filtered_matrices)\n",
    "    if n_matrices == 0:\n",
    "        print(\"No valid adjacency matrices to plot.\")\n",
    "        return\n",
    "\n",
    "    n_rows = int(np.ceil(np.sqrt(n_matrices)))\n",
    "    n_cols = int(np.ceil(n_matrices / n_rows))\n",
    "\n",
    "    # Create a large figure to accommodate all subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12 * n_cols, 10 * n_rows))\n",
    "    axes = axes.flatten() if n_matrices > 1 else [axes]\n",
    "\n",
    "    for idx, (ax, (title, adj_matrix)) in enumerate(zip(axes, filtered_matrices.items())):\n",
    "        simple_title = f'Protein: {title.split(\"_adj_matrix\")[0]}'\n",
    "        max_benign = adj_matrix[adj_matrix > 0].max().max()  # Max value among positive counts\n",
    "        max_pathogenic = -adj_matrix[adj_matrix < 0].min().min()  # Negative of min value among negative counts\n",
    "\n",
    "        # Handle potential NaNs if matrices are all zeros or have no valid data\n",
    "        vmax = max(max_benign if np.isfinite(max_benign) else 0, max_pathogenic if np.isfinite(max_pathogenic) else 0)\n",
    "        vmin = -vmax\n",
    "\n",
    "        sns.heatmap(adj_matrix, cmap='Spectral', center=0, vmin=vmin, vmax=vmax, ax=ax, robust=True, cbar=(mutation_type == 'indels' or (mutation_type == 'subs' and idx % n_cols == 2)))\n",
    "\n",
    "        if mutation_type == 'subs' and idx % n_cols == 2:\n",
    "            cbar = ax.collections[0].colorbar\n",
    "            cbar.set_ticks([vmin, 0, vmax])\n",
    "            cbar.set_ticklabels([f'Pathogenic\\ncounts', '0', f'Benign\\ncounts'], size=16)\n",
    "        elif mutation_type == 'indels':\n",
    "            cbar = ax.collections[0].colorbar\n",
    "            if cbar is not None:\n",
    "                cbar.set_ticks([vmin, 0, vmax])\n",
    "                cbar.set_ticklabels([f'Pathogenic\\ncounts', '0', f'Benign\\ncounts'], size=16)\n",
    "\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0)  # Rotate y-ticks for better visibility\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)  # Rotate x-ticks for better visibility\n",
    "        ax.set_title(simple_title, size=22)\n",
    "\n",
    "        # Set aspect ratio to equal\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "    # Hide any unused axes if there are any\n",
    "    for ax in axes[len(filtered_matrices):]:\n",
    "        ax.axis('off')\n",
    "    if mutation_type == 'subs':\n",
    "        plt.suptitle(overall_title, size=52, weight='bold')\n",
    "    elif mutation_type == 'indels':\n",
    "        plt.suptitle(overall_title, size=22, weight='bold')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.99, 0.98])  # Adjust layout to accommodate the title\n",
    "    sns.despine()\n",
    "    plt.savefig(plot_filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Plot the adjacency matrices for each protein\n",
    "plot_adjacency_matrices(subs_adjacency_matrices,\n",
    "                        plot_filename= plots_directory/\"subs_adjacency_matrices.png\",\n",
    "                        mutation_type='subs',\n",
    "                        overall_title='Substitutions')\n",
    "\n",
    "plot_adjacency_matrices(indels_adjacency_matrices,\n",
    "                        plot_filename=plots_directory/\"indels_adjacency_matrices.png\",\n",
    "                        mutation_type='indels',\n",
    "                        overall_title='Indels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_peptides_scores_by_protein(score_encoded_dataframe, output_file, mutation_type, title):\n",
    "    \"\"\"\n",
    "    Creates boxplots per protein showcasing the distribution of pathogenic and benign scores for the top 10 most frequent nullpeptides.\n",
    "    \"\"\"\n",
    "    if len(score_encoded_dataframe) == 0:\n",
    "        print('No scores to plot.')\n",
    "        return\n",
    "    \n",
    "    proteins = score_encoded_dataframe['Protein'].unique()\n",
    "    valid_datasets = len(proteins)  # The number of unique proteins determines the number of datasets\n",
    "\n",
    "    # Dynamically calculate rows based on the number of datasets\n",
    "    if mutation_type == 'subs':\n",
    "        cols = 3  # Define the number of columns in the subplot grid\n",
    "    elif mutation_type == 'indels':\n",
    "        cols = 1  # Define the number of columns in the subplot grid\n",
    "\n",
    "    rows = (valid_datasets + cols - 1) // cols  # Calculate the required number of rows\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 6, rows * 4))\n",
    "\n",
    "    # If there's only one subplot, wrap axes in a list to simplify the loop\n",
    "    if rows * cols == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()  # Flatten the array to simplify the loop\n",
    "\n",
    "    # Loop through each protein and create a plot\n",
    "    for i, (ax, protein) in enumerate(zip(axes, proteins)):\n",
    "        data = score_encoded_dataframe[score_encoded_dataframe['Protein'] == protein]\n",
    "        melted_data = data.melt(id_vars=['Nullpeptides'], value_vars=['Pathogenic_scores', 'Benign_scores'],\n",
    "                                var_name='Type', value_name='Score')\n",
    "        melted_data = melted_data.explode('Score')\n",
    "        melted_data['Score'] = melted_data['Score'].astype(float)\n",
    "        \n",
    "        # Replace the type values with 'Pathogenic' and 'Benign'\n",
    "        melted_data['Type'] = melted_data['Type'].replace({'Pathogenic_scores': 'Pathogenic', 'Benign_scores': 'Benign'})\n",
    "\n",
    "        # Create boxplot and stripplot\n",
    "        bp = sns.boxplot(x='Nullpeptides', y='Score', hue='Type', data=melted_data, palette='viridis', width=0.7, boxprops=dict(alpha=.8), fliersize=0, ax=ax)\n",
    "        sns.stripplot(x='Nullpeptides', y='Score', hue='Type', data=melted_data, palette='dark:k', size=3, jitter=True, dodge=True, alpha=0.5, ax=ax)\n",
    "\n",
    "        # Only show y-axis label on the first plot of each row\n",
    "        if i % cols == 0:\n",
    "            ax.set_ylabel('Protein Fitness', size=12)\n",
    "        else:\n",
    "            ax.set_ylabel('')\n",
    "\n",
    "        ax.set_xlabel('')\n",
    "\n",
    "        # Set the y-ticks with two decimal points\n",
    "        ax.yaxis.set_major_formatter(mticker.FormatStrFormatter('%.2f'))\n",
    "        if protein == 'YAP1':\n",
    "            ax.yaxis.set_major_locator(mticker.MultipleLocator(2.0))\n",
    "        else:\n",
    "            ax.yaxis.set_major_locator(mticker.MultipleLocator(0.5))\n",
    "\n",
    "        # Rotate the x-ticks for better readability using tick_params\n",
    "        ax.tick_params(axis='x', rotation=25)\n",
    "\n",
    "        # Remove legends from all but the top right plot\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "        ax.set_title(f'Protein: {protein}')\n",
    "    \n",
    "    if mutation_type == 'subs':\n",
    "        fig.suptitle(title, size=22, weight='bold')\n",
    "    if mutation_type == 'indels':\n",
    "        fig.suptitle(title, size=12, weight='bold')\n",
    "\n",
    "    # If there are fewer plots than axes, turn off the extra axes\n",
    "    for j in range(valid_datasets, rows * cols):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])  # Adjust layout to accommodate the legend\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')  # Save with tight bounding box\n",
    "    plt.show()\n",
    "\n",
    "# Plot boxplots for the top 10 most frequent peptides by protein\n",
    "plot_peptides_scores_by_protein(filtered_score_encoded_nullpeptides_subs, \n",
    "                                plots_directory/\"subs_box_plots.png\", \n",
    "                                mutation_type='subs',\n",
    "                                title='Substitutions')\n",
    "\n",
    "plot_peptides_scores_by_protein(filtered_score_encoded_nullpeptides_indels, \n",
    "                                plots_directory/\"indels_box_plots.png\", \n",
    "                                mutation_type='indels',\n",
    "                                title=' ')\n",
    "\n",
    "# Since NKX31 is possesses only one nullpeptide, we plot a separate boxplot, otherwise the result would be lost\n",
    "NKX31_data = score_encoded_nullpeptides_indels[score_encoded_nullpeptides_indels['Protein'] == 'NKX31']\n",
    "plot_peptides_scores_by_protein(NKX31_data,\n",
    "                                plots_directory/\"indels_NKX31_box_plots.png\",\n",
    "                                mutation_type='indels',\n",
    "                                title='Indels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count encode and plot the original unfiltered nullpeptides per protein to study the original distribution of pathogenic and benign nullpeptides\n",
    "def count_encode_nullpeptides_by_protein(original_dataframe):\n",
    "    \"\"\"\n",
    "    Processes a DataFrame by splitting it by 'Protein' and encoding nullpeptides based on their 'State' counts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the DataFrame into smaller DataFrames for each unique protein\n",
    "    protein_dataframes = {protein: group for protein, group in original_dataframe.groupby('Protein')}\n",
    "\n",
    "    # Initialize a list to collect results DataFrames\n",
    "    encoded_dfs = []\n",
    "\n",
    "    # Process each smaller DataFrame\n",
    "    for protein, data in protein_dataframes.items():\n",
    "        # Encode nullpeptides based on State counts for each split DataFrame\n",
    "        count_encoded_df = nullpeptide_encoding(data, 'Nullpeptides', 'State', encoding='count')\n",
    "        count_encoded_df['Protein'] = protein  # Add protein name to the DataFrame\n",
    "        encoded_dfs.append(count_encoded_df)\n",
    "\n",
    "    # Concatenate all results into a single DataFrame and reset the index\n",
    "    final_encoded_df = pd.concat(encoded_dfs).reset_index(drop=True)\n",
    "\n",
    "    return final_encoded_df\n",
    "\n",
    "# Count encode nullpeptides per protein\n",
    "count_encoded_nullpeptides_subs = count_encode_nullpeptides_by_protein(filtered_subs_df)\n",
    "count_encoded_nullpeptides_indels = count_encode_nullpeptides_by_protein(filtered_indels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aggregated_proportions(aggregated_data, mutation_type):\n",
    "    \"\"\"\n",
    "    Plots the stacked bar chart for the given aggregated data, sorting by pathogenic proportion.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort the data by Pathogenic proportion in descending order\n",
    "    aggregated_sorted = aggregated_data.sort_values(by='Pathogenic_proportion', ascending=True)\n",
    "\n",
    "    # Create a figure and one subplot\n",
    "    fig, ax = plt.subplots(figsize=(10, 18))\n",
    "    colors = sns.color_palette(\"viridis\")  # Set a color palette for the bars\n",
    "\n",
    "    # Plot horizontal stacked bars\n",
    "    bar_width = 0.8\n",
    "    ax.barh(aggregated_sorted['Protein'], aggregated_sorted['Pathogenic_proportion'], color=colors[2], label='Pathogenic', alpha=0.85, height=bar_width)\n",
    "    ax.barh(aggregated_sorted['Protein'], aggregated_sorted['Benign_proportion'], left=aggregated_sorted['Pathogenic_proportion'], color=colors[3], label='Benign', alpha=0.85, height=bar_width)\n",
    "\n",
    "    ax.set_yticks(range(len(aggregated_sorted)))\n",
    "    ax.set_yticklabels(aggregated_sorted['Protein'], fontsize=12)\n",
    "    ax.tick_params(axis='x', labelsize=12)\n",
    "    ax.set_xlabel('Proportion of Pathogenic and Benign Nullpeptides', size=14)\n",
    "    ax.axvline(x=0.5, color='black', linewidth=2, linestyle='dotted', alpha=0.6)\n",
    "    ax.set_xlim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Adjust alignment\n",
    "    for label in ax.get_yticklabels():\n",
    "        label.set_verticalalignment('center')\n",
    "\n",
    "    # Create a legend outside the plot area for 'subs' mutation type\n",
    "    if mutation_type == 'subs':\n",
    "        ax.set_title('Substitutions', size=34, weight='bold')\n",
    "\n",
    "    if mutation_type == 'indels':\n",
    "        legend = fig.legend(loc='upper right', bbox_to_anchor=(1.22, 0.05), bbox_transform=ax.transAxes,\n",
    "                            prop={'size': 14}, frameon=True, framealpha=0.9, fancybox=True)\n",
    "        ax.set_title('Indels', size=34, weight='bold')\n",
    "    \n",
    "    plt.savefig(f\"{plots_directory}/{mutation_type}_original_stacked_bar_plot.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Aggregate and calculate proportions for Indels\n",
    "indel_aggregated = count_encoded_nullpeptides_indels.groupby('Protein').agg({\n",
    "    'Pathogenic_counts': 'sum',\n",
    "    'Benign_counts': 'sum'\n",
    "}).reset_index()\n",
    "indel_aggregated['Total'] = indel_aggregated['Pathogenic_counts'] + indel_aggregated['Benign_counts']\n",
    "indel_aggregated['Pathogenic_proportion'] = indel_aggregated['Pathogenic_counts'] / indel_aggregated['Total']\n",
    "indel_aggregated['Benign_proportion'] = indel_aggregated['Benign_counts'] / indel_aggregated['Total']\n",
    "\n",
    "# Aggregate and calculate proportions for Substitutions\n",
    "substitution_aggregated = count_encoded_nullpeptides_subs.groupby('Protein').agg({\n",
    "    'Pathogenic_counts': 'sum',\n",
    "    'Benign_counts': 'sum'\n",
    "}).reset_index()\n",
    "substitution_aggregated['Total'] = substitution_aggregated['Pathogenic_counts'] + substitution_aggregated['Benign_counts']\n",
    "substitution_aggregated['Pathogenic_proportion'] = substitution_aggregated['Pathogenic_counts'] / substitution_aggregated['Total']\n",
    "substitution_aggregated['Benign_proportion'] = substitution_aggregated['Benign_counts'] / substitution_aggregated['Total']\n",
    "\n",
    "# Plot for Substitutions and Indels\n",
    "plot_aggregated_proportions(substitution_aggregated, 'subs')\n",
    "plot_aggregated_proportions(indel_aggregated, 'indels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
